\chapter{Introduction}




%
%
%
\chapter{Presets}
%
%
%
\section{Introduction to StarCraftII Reinforcement Learning}

\subsection{Game mechanics}

\subsection{SMAC - A StarCraftII multi-agent environment}

\subsection{PyMARL - A multi-agent reinforcement learning algorithm framework}

\section{Reinforcement Learning Basics}

\subsection{MDP - Markov Decision Process}

\subsection{Dec-POMDP - Decentralized Partially Observable MDP}
A Concise Introduction to Decentralized POMDPs.

Optimal and approximate q-value functions for decentralized pomdps

\section{Basic Reinforcement Learning Algorithms}


\subsection{Q-Learning}

helps with learning without the explicit information about the dynamics of the environment or the rewards. allows to estimate the value or quality of an action in a particular state of the environment.

\begin{center}
	$ Q_{i+1}(s_t, a_t)= (1-\eta_t)Q_i(s_t, a_t) +\eta_t(r_t+\gamma max_aQ_i(s_t, a_t))$
	
		$ \eta_t \in (0,1)$ is the learning rate
\end{center}


Learning from delayed rewards

\subsection{DQN - Deep Q-Network}

Model free, approximate Q-value function with CNN

\begin{center}
	Squared Temporal Difference (TD) Error:
	$ \mathcal{L}(\theta) = \sum_{i=1}^b [(y_i^{DQN} - Q(s,u;\theta))^2]$
\end{center}

\begin{center}
	$ y_i^{DQN} = r + \gamma max_{u^{'}}Q(s^{'},u^{'};\theta^-) $
\end{center}

where $\theta^-$ are parameters from a target network copying $\theta$ periodically
.\newline 
 
Reinforcement learning for robots using neural networks. 

Playing atari with deep reinforcement learning.

Human-level control through deep reinforcement learning. Nature,

Prioritized experience replay

\subsection{Actor-Critic}

\subsection{A3C - Asynchronous Actor-Critic Agents}

\subsection{BPTT - Truncated back-propagation through time }
%
%
%
\section{Problems in MARL}
%
%
%
\subsection{Credit Assignment Problem}

Because the TD error considers only
global rewards, the gradient computed for each actor does
not explicitly reason about how that particular agents actions
contribute to that global reward. Since the other agents
may be exploring, the gradient for that agent becomes very
noisy, particularly when there are many agents. -COMA

\subsection{Stability of experience replay}

\subsection{Non-stationarity of the environment}

Changing policies of agents affect each others learning-> no convergence
exploration, 

Agents are part of other agents environment - MACKRL Tan

\subsection{Centralized learning of decentralised policies}

decentralised policies = executed independently = policy conditions on agents own action-observation history

learned centralised (work (Rashid et al., 2018; Foerster et al.,
22016, 2017, 2018; Kraemer Banerjee, 2016; Jorge et al., 2016),)

\subsection{Lazy agent problem}
 TODO
- "The distributed nature of the learning offers new benefits but also challenges such
as the definition of good learning goals or the convergence and consistency of algorithms [Sch14, BBDS08]" IQL

- "multiagent case ,environment state transitions and rewards are
affected by  joint action of all agents. -> value of an agent?s action depends on actions of others -> each agent must keep track of each of other learning agents, possibly resulting in an ever-moving target. In general, learning in the presence of other agents
requires a delicate trade-off between the stability and adaptive behavior of each agent." IQL

-"decentralised poli-
cies, which severely limit the agents? ability to coordinate their behaviour."-MACKRL

"Often agents are forced to ignore information in their individual observations that
would in principle be useful for maximising reward, because acting on it would make their behaviour
less predictable to their teammates. This limitation is particularly salient in IL, which cannot solve
many coordination tasks (Claus Boutilier, 1998)."
%
%
%
¸\section{Motivation for multi-agent reinforcement learning}

single agents typically fare
poorly on such tasks, since the joint action space of the
agents grows exponentially with the number of agents. - COMA
%
%
%
\chapter{Related work on MARL}
%
%
%
\section{JAL - Joint Action Learning}
Claus Boutilier 1998 - MACKRL
\section{IL - Independent Learning}
Tan 1993 - MACKRL
%
%
%
\section{IQL - Independent Q-Learning}
Two agents controlled by independent Deep Q-Networks as described in [MKS+15], "simplest method consists of using an autonomous Q-learning algorithm for each agent
in the environment, thereby using the environment as the sole source of interaction between agents."
%
%
%
\section{VDN - Value-decomposition Networks}
The main idea behind VDN arises from the assumption that joint-action-value functions can be decomposed into $ \mathit{d} $ single-agent value functions which are then composed via summation into the join-action-value function.

\begin{center}
	$ Q((h^1, h^2, ..., h^d),(a^1, a^2, ..., a^d)) \approx \sum_{i=1}^{d} \tilde{Q}_i(h^i, a^i) $
\end{center}

Thus each agents value-function $ \tilde{Q}_i $ depends solely on it`s local observation. The history of agent $\mathit{i}$ is denoted as $h^i$ while $a^i$ corresponds to the agent`s current action. The history $h_t = a_1o_1r_1,...,a_{t-1}o_{t-1},r_{t-1}$ of an agent includes it`s action, observation and reward at a given timestep up until a the last registered timestep ${t-1}$. 

The agent`s value-function is learned via back-propagation of the gradients resulting from applying the Q-learning rule on the joint reward and the local observations. $ \tilde{Q}_i $ is therefore independent of specific rewards.

As a result each agent independently performs greedy actions based on it`s own $ \tilde{Q}_i $.
%
%
%

\section{QMIX - Monotonic Value Function Factorisation}
The basic idea behind QMIX results from the insight that full factorisation as performed in VDN can be replaced with a constraint on the joint-action-value function:

\begin{center}
	$ argmax_u Q() = \dottedcolumn{3}{argmax_u^1 Q_1()}{argmax_u^n Q_n()}$
\end{center}
TODO: explain constraint

Further monotonicity is required due the constraint:

\begin{center}
	$ \frac{\partial Q}{\partial Q_a} \geq 0, \forall a \in A$
\end{center}

In order to fulfill the above requirements, various agents networks as well as a hypernetwork and mixing network are deployed.
Each agent network is comprised as a DRQN representing its individual value function $Q_a$. This network is provided with the current observation $o_t^a$ and the last action $u_{t-1}^a$ of agent $a$ at each timestep.

The mixing network receives the agent network outputs and returns $Q$ by mixing the outputs monotonically. The monotonicity contraint is enforced via non-negative weights in the mixing layer.

The weights (of one layer??) in return are produced by hypernetworks which are supplied with the state. This networks consist of a single linear layer and a absolute function (non-negative weights). Biases are independently generated.

%
%
%
\section{COMA - Counterfactual Multi-Agent Policy Gradients}

actor-critic policy-gradient method, decentralised policies, centralised critic to estimate Q-function and decentralised actors to optimise agents policies. counterfactual baseline marginalises single agents action while keeping other agents actions fixed.
%
%
%
\section{Central-V}
%
%
%
\section{ASN - Action Semantic Networks}
%
%
%
\section{MACKRL - Multi-Agent Common Knowledge Reinforcement Learning}

"stochastic actor-critic algorithm that learns a hierarchical policy tree."

higher tree levels = groups
lower levels = subgroups (richer common knowledge)
lowest level = independently learnt decentralised policies


"centralised training of decentralised policies.
During learning the agents can share observations, parameters, gradients, etc. without restriction but
the result of learning is a set of decentralised policies such that each agent can select actions based
only on its individual observations"

\subsection{Common Knowledge Concept}
Agents share common knowledge as soon as the see each other defined on their field of view.

Without common knowledge, decentralised coordination of agents has to fall back on implicit communication. This includes observation of actions and their resulting effects in the environment and other agents.

problems of implicit:
"typically
require multiple timesteps to execute, can limit the agility of control during execution (Tian et al.,
2018)"

\iffalse
Das ist die Ebene {\tt subsubsection}.

\subsubsection{Noch ein Unterunterabschnitt}
\label{sec:b}

Wer \ref{sec:a} sagt, mu"s auch \ref{sec:b} sagen.

\subsection{Noch ein Unterabschnitt}

Das ist ein gew"ohnlicher Absatz.

\paragraph{Ein Absatz mit Titel}

Das ist die Ebene {\tt paragraph}.

\subparagraph{Ein Unterabsatz mit Titel}

Das ist die Ebene {\tt subparagraph}.

\subsection*{Ein nicht numerierter Unterabschnitt}

Dieser Unterabscnitt erscheint nicht im Inhaltsverzeichnis.

\section{Beispiele}
Dieses Beispieldokument wurde mit dem \LaTeX-Paket {\tt dbstmpl.sty} erzeugt.
{\tt dbstmpl.sty} definiert Befehle zum Erzeugen von einigen
h"aufig gebrauchten Symbolen.

\begin{align}
  \N \subset \Z \subset \Q \subset \R \subset \C
\end{align}

Weitere Symbole k\"onnen im {\tt symbols-a4.pdf}-Dokument, das f\"ur
gew\"ohnlich bei \LaTeX-Distributionen mitgeliefert wird, nachgeschlagen
werden. Alternativ gibt es diverse Hilfeseiten im Netz.

Au"serdem werden automatisch einige zus"atzliche Pakete geladen.
Mit dem Paket {\tt graphicx} k"onnen Grafiken eingebunden werden.
Abbildung~\ref{fig:dbslogo} zeigt das Logo der LFE f"ur Datenbanksysteme.

\begin{figure}
  \begin{center}
    \includegraphics[width=2cm]{logo-dbs.png}
  \end{center}
  \caption{Altes Logo der LFE f"ur Datenbanksysteme.
    \label{fig:dbslogo} % Figures und Tables können gelabelt werden.
    % Es empfiehlt sich, dies in der Caption zu machen
  }
\end{figure}

{\tt amsmath} erlaubt die bequeme Alignierung von mehrzeiligen Formeln:

\begin{align}
  f(a,b) &= (a-b)^2 \label{eq:aMinusbSq}\\
    &= a^2 - 2 a b + b^2 \label{eq:aMinusbSqResolved}
\end{align}

Zus\"atzlich k\"onnen diese Formeln auch referenziert werden. In~\eqref{eq:aMinusbSq}
wird das Problem gestellt und in~\eqref{eq:aMinusbSqResolved} wird es etwas
aufgel\"ost. Nat\"urlich k\"onnen die Gleichungen auch ohne Klammern
referenziert werden: \ref{eq:aMinusbSq}, \ref{eq:aMinusbSqResolved}.

Das in diesem Dokument geladene Paket {\tt subfigure} (oben eingebunden durch
\verb=\usepackage{subfigure}=) erlaubt das Erzeugen von Unterabbildungen.
Abbildung~\ref{fig:logos} enth"alt zwei Unterabbildungen.
Abbildung~\ref{fig:logos-a} zeigt nochmal das Logo der LFE f"ur
Datenbanksysteme, \ref{fig:logos-b} zeigt das Logo der
Ludwig-Maximilians-Universit"at.

\begin{figure}
  \begin{center}
    \subfigure[Altes DBS-Logo.\label{fig:logos-a}]{
      \makebox[0.48\textwidth]{
        \includegraphics[width=2cm]{logo-dbs.png}}}
    \hspace{\fill}
    \subfigure[Uraltes LMU-Logo.\label{fig:logos-b}]{
      \makebox[0.48\textwidth]{
        \includegraphics[width=5cm]{logo-lmu.png}}}
  \end{center}
  \caption{Zwei Logos.}
  \label{fig:logos}
\end{figure}

Tabelle~\ref{tab:quadratzahlen} zeigt die ersten f"unf Quadratzahlen.

\begin{table}
  \begin{center}
  \begin{tabular}{|r|r|}
    \hline \makebox[1cm]{$x$} & \makebox[1cm]{$x^2$}\\
    \hline\hline 1 &  1\\
    \hline 2 &  4\\
    \hline 3 &  9\\
    \hline 4 & 16\\
    \hline 5 & 25\\
    \hline
  \end{tabular}
  \end{center}
  \caption{Die Zahlen 1 bis 5 und ihre Quadrate.}
  \label{tab:quadratzahlen}
\end{table}

Folgenderma\ss en wird Literatur referenziert.
DBSCAN~\cite{EKSX96} und OPTICS~\cite{ABKS99} sind Beispiele f\"ur
dichtebasierte Clusteringverfahren. Diese Eintr\"age werden als
{\tt bibitem} im {\tt .bbl}-File referenziert. Um dieses File braucht man
sich erfreulicherweise nicht zu k\"ummern, weil es automatisch aus einer
Sammlung von BibTeX-Eintr\"agen in einem Literatur-File gebildet werden
kann. Hier ist wichtig, dass dieses File den BibTeX-Konventionen
\footnote{{\tt http://www.tex.ac.uk/tex-archive/biblio/bibtex/contrib/doc/btxdoc.pdf}}
entspricht, sonst kommen kreative Fehlermeldungen. Daf\"ur braucht es
besagte Literatur-Sammlung -- hier {\tt dbstmpl.bib}
und einen Literatur-Stil, den man auch selber definieren kann, wie
etwa {\tt dbstmpl.bst}.

\section{Bemerkungen}

Eine "Ubersicht "uber alle Abbildungen und Tabellen einer Arbeit
verschaffen das Abbildungs- und das Tabellenverzeichnis. Diese
sollten entweder nach dem Inhaltsverzeichnis oder (wie in diesem
Dokument) vor dem Literaturverzeichnis eingef"ugt werden.
Je nach Bedarf und Umfang der Arbeit k"onnen auch Verzeichnisse
f"ur Definitionen, S"atze oder Lemmata angelegt oder nicht
ben"otigte Verzeichnisse weggelassen werden.

\appendix

\chapter{Ein Anhang}

Im Anhang kann auf Implementierungsaspekte wie Datenbankschemata
oder Programmcode eingegangen werden.

\section{\protect{pdflatex} vs.\ \protect{latex}}

Dieses Dokument wird mit {\tt pdflatex} gebaut. Das ist n\"otig, wenn
wie hier Bilder im {\tt .jpg}, {\tt .png} oder {\tt .pdf}-Format eingebunden
werden. Alternativ kann auch alles \"uber das klassische \LaTeX-Kommando
laufen: \verb=latex dbsba=. Damit wird ein {\tt .dvi} erstellt, welches
erst mit {\tt dvipdf} zu {\tt .pdf} konvertiert werden muss. Dies
bedeutet, dass Bilder nur noch als {\tt .eps} eingebunden werden
k\"onnen. {\em Merke: {\tt .pdf}-affine und {\tt .eps}-Bilder k\"onnen
NICHT kombiniert werden, also entweder oder.}

\section{Ich will aber englisch!}

Ihr d\"urft auch gerne englische \the\arbeit en schreiben. Daf\"ur m\"usst
ihr im Stylefile {\tt dbstmpl.sty} die Zeile
\verb=\RequirePackage[english,german]{babel}= durch
\verb=\RequirePackage[german,english]{babel}= austauschen. Dann hei\ss en
Chapter wieder Chapter und nicht Kapitel, etc.

\section{Ich h\"atte da einen eigenen Stil.}

Ihr k\"onnt auch eigene Stile definieren und verwenden, wenn er die
Informationen des Deckblatts und die darauffolgende Erkl\"arung
enth\"alt, sowie die Gnade des Betreuers gefunden hat (es
empfiehlt sich also, fr\"uhzeitg kurz nachzufragen).

\section{Hilfe?}

Gibt es online zuhauf:

\begin{description}
  \item[Das \LaTeX Kochbuch]~\\{\tt http://www.uni-giessen.de/hrz/tex/cookbook/cookbook.html}
  \item[CTAN] {\tt http://www.ctan.org/} zum manuellen Download von Stylefiles
      und deren Dokumentation
  \item[Diverse Mathe-Tweaks]~\\ \verb=http://meta.wikimedia.org/wiki/Help:Displaying_a_formula=
  \item[B\"uchersuche mit BibTeX -Unterst\"utzung] {\tt http://lead.to/amazon/en/}
  \item[JabRef] Java-basierter BibTeX -Manager {\tt http://jabref.sourceforge.net/}
  \item[FAQs] {\tt http://www.tex.ac.uk/cgi-bin/texfaq2html}
\end{description}

Und zur Not versteht auch euer Betreuer vermutlich was davon.
\fi