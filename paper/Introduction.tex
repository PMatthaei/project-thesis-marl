\chapter{Introduction}

TODO:

- What is Multi-Agent Reinforcement Learning (MARL)

\chapter{Presets}

\section{PyMARL - A multi-agent reinforcement learning framework}

In order to setup PyMARL(fork) with extensions on local and combined rewards, the Readme in the following repository \url{https://github.com/PMatthaei/pymarl} contains a step by step guide. The main repository of PyMARL can be found at \url{https://github.com/oxwhirl/pymarl}. \cite{samvelyan19smac}

TODO:

- Why these two algorithms only?

- What are they doing?

- What are problems?

\subsection{Value Decompostion Networks}
\subsection{QMIX - Monotonic Value Function Factorisation}

\section{StarCraftII Multi Agent Challenge Environment}

PyMARL`s multi-agent reinforcement learning algorithms are running in the SMAC environment \url{https://github.com/oxwhirl/smac}. \cite{samvelyan19smac} The following sections are disecting the construction of the reward in SMAC.

\subsection{Reward function}

The StarCraftII Multi Agent Challenge (SMAC) environment supports sparse and dense reward functions. This can be controlled via the configuration parameter \verb|reward_sparse|. Per default sparse rewards are set to \verb|False|, resulting in dense rewards for all agents interacting with the environment. In the following other important configuration parameters regarding the reward function and their default values are listed:
\begin{itemize}
	  
	\item \verb|reward_death_value: -10|
	\newline Reward for death of a unit.
	
	\item \verb|reward_defeat: 0|
	\newline Reward for losing a round.
	
	\item \verb|reward_negative_scale: 0.5|
	\newline Scale negative rewards to count just half as much as positive rewards.
	
	\item \verb|reward_only_positive: True|
	\newline Do not reward unit death. %TODO shield regen?
		
	\item \verb|reward_scale_rate: 20|
	\newline Scale rate to normalize rewards. (20 is the maximum achievable reward)
	
	\item \verb|reward_win: 200|
	\newline Reward for winning a round.
	
\end{itemize}

The function \verb|reward_battle| instantly returns zero if \verb|reward_sparse| is set to \verb|True| since battle actions are only used for sparse rewarding. Rewards concerning overall win and loss are added during the step implementation of the environment. In the following description of the reward construction only alive units are considered.

The sparse (battle) reward is constituted by the accumulative hit/shield point damage dealt to 
all enemy units as well as the \verb|reward_death_value| for each killed enemy.
In case only positive rewards are configured, allied casualties are taken into account. Hereby the damage dealt to ally units and \verb|reward_death_value| per ally unit killed are scaled by \verb|reward_negative_scale| and afterwards deduced from the reward. Therefore received damage of allies and their death is penalized.

Since the reward is accumulated, the contribution of each unit/agent can not be deduced afterwards. The python code corresponding to the global reward function can be found in the appendix in \autoref{sec:reward_global_python_code}.

\section{Global and local reward functions}\label{sec:global_local_r_function}
The global reward function $R^{global}(s, a)=\dfrac{1}{n} \sum_{i=1}^{n} R_i\left(s^{(i)}, a^{(i)}\right)$ in a multi-agent setup is defined as a function returning the total reward for a group of \textit{n} agents. Each agent is provided with this reward as result of his own action contributing to the joint action.

On the other side, a local reward function $R_{i}^{local}\left(s^{(i)}, a^{(i)}\right)$ should reward an action taken by a single agent \textit{i} within a group. Accumulating all local rewards of agents in the same group should result in the global reward calculated for this group. \cite{NIPS2005_2951}

\subsection{Rewards in Multi-Agent Reinforcement Learning}
\cite{NIPS2005_2951} Shows that most algorithms "will need to see $\tilde{\Omega}(n)$ trajectories in the worst case to achieve good performance". An important definition arising from this paper is the neighborhood of an agent i, which describes the subset of agents that are directly influenced by agent i`s state.

\subsubsection{Credit Assignment Problem}
% TODO: Describe problem via sources
In case a group of agents is producing a joint or collective action in an environment, a single reward signal loses information about each agents contribution to the reward. In this scenario, the credit assignment problem is targeting the question which agents should receive more credit for positively contributing to the global reward signal. \cite{sutton2018reinforcement}

% TODO: which algorithms have TD error? maybe qtran coma etc do not use it and are therefore bad for comparsion
For learning algorithms relying on the TD error computed on the global reward - such as VDN - the obtained gradient for each agent does not provide information about how this agents contribution to the reward. With increasing amount of agents, the gradient becomes noisy due to the fact that some agents might be exploring.
\cite{foerster2018counterfactual}

\subsection{Challenges in reward shaping}
% TODO: Describe problems arising from credit assignment. When is reward justified? -> complex and heavily depends on game mechanics

\chapter{Dynamic reward composition}
\section{Idea}

Since the global reward function can de decomposed into a sum of agent-specific local reward functions we can choose arbitrary mixins of these functions. In other words, there is the possibility to choose which reward signal is predominantly influencing the agents policy. Agents which rely more on the local reward may suffer less from the credit assignment problem but are prone to acting selfishly in situations where team work is necessary.

Agents focusing more on global rewards for their training procedure are denoted as "altruistic" or "selfless" since their actions will be choosen on the intention of increasing profit for their team.

Agents concentrating on local rewards for their training procedure are denoted as "selfish" since their actions will be choosen in a way to increase their own profit.

The agents reward focus can be set via a combination weight $c \in [0,1]$. A combination weight of $c=1$ constitutes a totally "self-interested" agent while choosing $c=0$ observes team performance only.
\section{Implementing a local reward function}

In order to prove the effectiveness of dynamic reward composition, a local reward function is needed in the SMAC environment. Therefore the current global reward function has to be analyzed to identify reasonable decompositions. It should be mentioned that reward shaping is a difficult topic and most algorithms run on simple global reward functions within their environment to prove independence of reward shaping since their target is to solve the problem without encoding the solution completely into the reward function. The implementation of a local reward function in this paper is targeting to be close to the global reward function of the SMAC environment to allow for comparsion of already conducted experiments. Therefore the following decisions have been made.
 
Each agent receives only his own health difference to the previous step as positive reward. This is very close to the global implementation where all allied units are considered.

During action selection, each agent which executed an attack action on an enemy is registered. Based on this targeting data, the total dealt damage and kill reward in a step is evenly split between all attacking agents. Passive agents which do not attack will not receive reward. Since the environment is deterministic in its attack damage calculation, the same amount of damage can be expected by the same unit in a step. This assumption does not hold with heterogeneous team compositions which is one reason to exclude them from the following experiments.

Natural global rewards such as round loss or win are evenly distributed between all agents.

The python code corresponding to the local reward function  can be found in the appendix in \autoref{sec:reward_local_python_code}.

\section{Combining local and global rewards}

In a group of \textit{n} cooperating agents, each agent \textit{i} chooses an action \newline 
$A = (a_{1},a_{2},...,a_{i}), i = n$ and is assigned his own reward combination weight $W_{reward} = (w_{1},w_{2},...,w_{i})$. Since the global reward is composed of the local rewards as depicted in \autoref{sec:global_local_r_function}, local and global rewards can be linearly mixed. Via the combination weight $w_{i}$ the combined reward can be shaped to reinforce local or global rewards. Thus the combined reward for an agent \textit{i} is

% TODO: is it really s_i? not the whole state of all agents since we are using them to calculate absolute damage etc

\begin{center}
	$R_{i}^{combined}\left(s^{(i)}, a^{(i)}\right) = w_{i} * R_{i}\left(s^{(i)}, a^{(i)}\right) + \dfrac{1}{n} * \sum_{j=1}^{n} (1-w_{j}) * R_{i}\left(s^{(i)}, a^{(i)}\right) $
\end{center}

Â¸where $w_{i}$ is the combination weight for the currently observed agent and $w_{j}$ are weights from group members of agent \textit{i}. The recombined global reward is therefore:

\begin{center}
	$R_{global}^{combined} = \dfrac{1}{n} * \sum_{j=1}^{n} (1-w_{j}) * R_{i}\left(s^{(i)}, a^{(i)}\right) $
\end{center}

The following experiments have been conducted with a uniform weight for all agents resulting in $w_{1}=w_{2}=...=w_{i}=w$. Therefore $R_{global}^{combined}$ can be rewritten to

\begin{center}
	$R_{global}^{combined} = \dfrac{1}{n} * \sum_{j=1}^{n} (1-\mathbf{w}) * R_{i}\left(s^{(i)}, a^{(i)}\right) = (1-w) * \dfrac{1}{n} * \sum_{j=1}^{n}  * R_{i}\left(s^{(i)}, a^{(i)}\right) = (1-w) * R^{global}(s, a)$
\end{center}

Resulting in the final form:

\begin{center}
	$R_{i}^{combined}\left(s^{(i)}, a^{(i)}\right) = w * R_{i}\left(s^{(i)}, a^{(i)}\right) + (1-w) * R^{global}(s, a) $
\end{center}

The python code corresponding to the combined reward function (with non-uniform weights) can be found in the appendix in \autoref{sec:reward_combination_code}.
\chapter{Evaluation}
\section{Experiments}

%TODO:
%- Compare VDN 3,8,25m with combined version c= 0,0.5,1(12 Experiments)
%- Compare QMIX 3,8,25m with combined version c= 0,0.5,1(12 Experiments)
%- Compare IQL 3,8,25m with combined version c= 0,0.5,1(12 Experiments)

VDN deployed in the 8m environment is already performing nearly perfectly although the learned policy differs noticebly from QMIX. Setting the combination weight to $c=0$ results in the best performance, indicating that a global reward signal is influencing a more effective policy and most prominent a faster learning. Agents trained with a combination weight of $c=1$ reach about a 80 percent win rate, but start learning when other algorithms already converge. Experiments conducted with $c=0.5$ show a fluctuating learning process, which nonetheless seems to converge to a similar performance. 

%TODO: why fluctuating?

\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{img/results/vdnc_8m/battle_won_mean.png}
		\caption{Battle won mean}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{img/results/vdnc_8m/return_mean.png}
		\caption{Return mean}
	\end{subfigure}
	
	\caption{VDN on 8 marines vanilla vs. combined rewards c=0.0,0.5,1.0.}
	\label{fig:vdn_8m_results}
\end{figure}

As expected, environments with a larger group of agents profit from local rewards and gain even more from combined rewards. Vanilla VDN and combined reward VDN with $c=0$ are not able to win any rounds. Providing the agents with local rewards only ($c=1$) allows for at least some wins. 
%TODO: why just max 4% wins
%TODO: why fluctuating win rate
%TODO: why so late?
%TODO: why return c=0 != return vanilla
\begin{figure}[h!]
	  \centering
	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{img/results/vdnc_25m/battle_won_mean.png}
		\caption{Battle won mean}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{img/results/vdnc_25m/return_mean.png}
		\caption{Return mean}
	\end{subfigure}

	\caption{VDN on 25 marines vanilla vs. combined rewards c=0.0,0.5,1.0.}
	\label{fig:vdn_25m_results}
\end{figure}


All models and experiment metrics can be found at \url{https://github.com/PMatthaei/pymarl-results}. Further experiment permutations than the ones included in this documentation can be evaluated via the \href{https://github.com/PMatthaei/pymarl-results/blob/master/visualize_pymarl.ipynb}{visualization script in Google Colaboratory}.


\chapter{Conclusion}
 Problem: Environment -> only homogeneous agents and symmetric groups. heterogeneous groups require more selfless behaviour f.e. tanking, exploding, being the bait.
 
\chapter{Future work}
The current implementation only used uniform unchangeable combine weights. Depending on the situation a agent might want to vary its combination to encourage a more selfish or more selfless behaviour.

Furthermore each agent should hold its own combination weight since their state may differ from other agents and requires a corresponding reward combination.

The current local reward function does not support heterogeneous team compositions since it is assumed every unit inflicts the same damage per step. This renders our implementation useless for a majority of environments.

Hyperparameter tuning not conducted.

\appendix
\chapter{Appendix}
\newpage
\section{SMAC - global reward function}
\label{sec:reward_global_python_code}
	\begin{python}
def global_reward(self):
	if self.reward_sparse:
		return 0
	
	reward, delta_deaths, delta_ally, delta_enemy = 0
	
	neg_scale = self.reward_negative_scale
	# update deaths
	for al_id, al_unit in self.agents.items():
		if not self.death_tracker_ally[al_id]:
		# did not die so far
		prev_health = (self.previous_ally_units[al_id].health			+ self.previous_ally_units[al_id].shield)
	if al_unit.health == 0:
		# just died
		self.death_tracker_ally[al_id] = 1
		if not self.reward_only_positive:
			delta_deaths -= self.reward_death_value * neg_scale
			delta_ally += prev_health * neg_scale
		else:
		# still alive
			delta_ally += neg_scale * (prev_health - al_unit.health - al_unit.shield)
	
	for e_id, e_unit in self.enemies.items():
		if not self.death_tracker_enemy[e_id]:
			prev_health = (
			self.previous_enemy_units[e_id].health
			+ self.previous_enemy_units[e_id].shield
			)
		if e_unit.health == 0:
			self.death_tracker_enemy[e_id] = 1
			delta_deaths += self.reward_death_value
			delta_enemy += prev_health
		else:
			delta_enemy += prev_health - e_unit.health - e_unit.shield
	# shield regeneration
	if self.reward_only_positive:
		reward = abs(delta_enemy + delta_deaths)  
	else:
		reward = delta_enemy + delta_deaths - delta_ally

return reward
	\end{python}
\newpage
\section{SMAC with combined rewards - local reward function} \label{sec:reward_local_python_code}
	\begin{python}
def local_reward(self, a_id, target_id):

	if self.reward_sparse:
		return 0

	neg_scale = self.reward_negative_scale
	delta_death, delta_self, delta_enemy = 0	
	agent_unit = self.get_unit_by_id(a_id)	
	# If the unit with id a_id is still alive
	if not self.death_tracker_ally[a_id]:
		# Fetch its previous health
		prev_health = self.previous_ally_units[a_id].health + self.previous_ally_units[a_id].shield
		# If it just died
		if agent_unit.health == 0:
		# Reward death negatively if configured
		if not self.reward_only_positive:
			delta_death -= self.reward_death_value * neg_scale
		# Remember lost health (= damage from enemies) to reward negatively later
			delta_self += prev_health * neg_scale
	# If still alive
	else:
		current_health = agent_unit.health + agent_unit.shield
		health_reward = prev_health - current_health
		# Reward the delta health (shield regeneration, heal through MMM)
		delta_self += neg_scale * health_reward
	
	# Search for the target (if it exists) which a_id attacked
	if target_id is not None:
		e_id, e_unit = next((e_id, _) for e_id, _ in self.enemies.items() if target_id == e_id)
		if not self.death_tracker_enemy[e_id]:
			# Reward dealt attack damage + kill
			delta_enemy += self.local_attack_r_t
	
	if self.reward_only_positive:
		reward = abs(delta_enemy + delta_death)
	else:
		reward = delta_enemy + delta_death - delta_self
	
	return reward
	\end{python}
\newpage
\section{Generalized reward combination function} \label{sec:reward_combination_code}
	\begin{python}
def combine_local_rewards(self, rs):
	ws = self.local_reward_weights
	# Pair local rewards with their weight
	rs_ws = list(zip(rs, ws))
	# Calculate weighted global reward
	r_global_combined = np.mean([(1.0 - w_j) * r_j for r_j, w_j in rs_ws])
	# Calculate each agents recombined reward
	rs_combined = [w_i * r_i + r_global_combined for r_i, w_i in rs_ws]
	
	return rs_combined
	\end{python}
